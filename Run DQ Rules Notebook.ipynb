{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f708ef4a-bf76-46b7-b520-8ae371ee47d1"
   },
   "source": [
    "# Data Quality Rules Execution\n",
    "\n",
    "## Overview\n",
    "This notebook executes IBM Cloud Pak for Data (CPD) data quality rules in parallel for improved performance. It provides:\n",
    "\n",
    "- **Parallel execution** of multiple DQ rules using ThreadPoolExecutor\n",
    "- **Configurable batch sizes** to control cluster load\n",
    "- **Output saved to project** with execution results  \n",
    "\n",
    "## Settings\n",
    "\n",
    "**CPD Host**: The IBM Cloud Pak for Data cluster endpoint where your data quality rules are deployed.\n",
    "\n",
    "**Batch Size**: Maximum number of rules to execute concurrently. Adjust based on your cluster capacity.\n",
    "**Delay in Seconds**: The number of seconds to wait between batches.\n",
    "\n",
    "\n",
    "> ⚠️ **Note**: Higher batch sizes may overwhelm the cluster and cause timeouts. Start with 5 and adjust based on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc98de88-ff44-4611-9954-8b6e8722c6c9"
   },
   "outputs": [],
   "source": [
    "CPD_HOST = \"cpd-host.company.com\"\n",
    "BATCH_SIZE = 5\n",
    "DELAY_IN_SECONDS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "643d102e-7570-4d71-8cd1-61cab5c24aa6"
   },
   "source": [
    "## Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "47d30135-cb08-421e-a1bd-636c857be5ff"
   },
   "outputs": [],
   "source": [
    "from ibm_watson_studio_lib import access_project_or_space\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03589a74-30a3-4542-9da5-4717fbe175bf"
   },
   "source": [
    "### Initialize Watson Studio library for project access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "09fcadba-8a1a-4bbf-9f5e-c6cd50ef44a0"
   },
   "outputs": [],
   "source": [
    "wslib = access_project_or_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2fe6448-8c2c-4726-a91f-d725e3250978"
   },
   "source": [
    "### Listing assets by a specific asset type\n",
    "You can use `wslib.assets.list_assets` without a filter to retrieve all assets of a given asset type. Use `wslib.assets.list_asset_types` to get a list of all available asset types. Or you can use the generic asset type asset to retrieve all assets. In this case we need data quality rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c091dc71-2c28-412f-85d8-651873c5a45f"
   },
   "outputs": [],
   "source": [
    "dq_rules = wslib.assets.list_assets(\"data_rule\")\n",
    "# wslib.show(dq_rules)\n",
    "# for dq_rule in dq_rules:\n",
    "#    print(dq_rule['asset_id'] + ' - ' + dq_rule['name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f92be881-19ba-47f9-a155-9ac4a563d220"
   },
   "source": [
    "## Core Execution Function\n",
    "\n",
    "Executes a single data quality rule via CPD API and returns structured results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "494ee13d-a062-4cbf-9abb-5de6581f6ec5"
   },
   "outputs": [],
   "source": [
    "def execute_rule(token, project_id, rule_id):\n",
    "    try:\n",
    "        url = f\"https://{CPD_HOST}/data_quality/v3/projects/{project_id}/rules/{rule_id}/execute\"\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Authorization': f'Bearer {token}'\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=headers, verify=False)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            execution_data = response.json()\n",
    "            return {\n",
    "                'success': True,\n",
    "                'rule_id': rule_id,\n",
    "                'rule_name': execution_data.get('name', 'Unknown Rule'),\n",
    "                'status': execution_data.get('status', {}).get('state', 'unknown'),\n",
    "                'job_id': execution_data.get('job', {}).get('id'),\n",
    "                'job_run_id': execution_data.get('job_run', {}).get('id')\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'rule_id': rule_id,\n",
    "                'error': f\"HTTP {response.status_code}\",\n",
    "                'response': response.text\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'rule_id': rule_id,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77654e63-08ea-4377-ad83-f28beba3f6f7"
   },
   "source": [
    "## Parallel Execution Function\n",
    "\n",
    "Executes multiple data quality rules concurrently using ThreadPoolExecutor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e9c7abcc-180e-4605-996f-3ac84de72e40"
   },
   "outputs": [],
   "source": [
    "def execute_rules_parallel(token, project_id, dq_rules, max_workers=5):\n",
    "    \"\"\"Execute rules in parallel and return structured results\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all jobs\n",
    "        future_to_rule = {\n",
    "            executor.submit(execute_rule, token, project_id, rule['asset_id']): rule\n",
    "            for rule in dq_rules\n",
    "        }\n",
    "        \n",
    "        # Process as they complete\n",
    "        for future in as_completed(future_to_rule):\n",
    "            rule = future_to_rule[future]\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append((rule, result))\n",
    "            except Exception as exc:\n",
    "                results.append((rule, {'success': False, 'error': str(exc)}))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71462980-12f1-4169-9d73-56eb980846a3"
   },
   "source": [
    "## Parallel Execution Function with Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "37a697a0-5e8a-4096-b5ac-b03cc04965ac"
   },
   "outputs": [],
   "source": [
    "def execute_rules_parallel_with_delay(token, project_id, dq_rules, batch_size=5, delay_between_batches=1):\n",
    "    \"\"\"Execute rules in parallel with delay in between\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    # If no delay requested\n",
    "    if delay_between_batches == 0:\n",
    "        return execute_rules_parallel(token, project_id, dq_rules, max_workers=batch_size)\n",
    "    \n",
    "    # Otherwise, process in batches with delays\n",
    "    for i in range(0, len(dq_rules), batch_size):\n",
    "        batch = dq_rules[i:i + batch_size]\n",
    "        batch_results = execute_rules_parallel(token, project_id, batch, max_workers=batch_size)\n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        if i + batch_size < len(dq_rules):\n",
    "            time.sleep(delay_between_batches)\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43c91fa6-733e-45f3-8279-8a9ace85fab8"
   },
   "source": [
    "## Results Processing\n",
    "\n",
    "Converts execution results into a pandas DataFrame for analysis and display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "db67fa7e-32e7-4f69-b37a-0003bc128189"
   },
   "outputs": [],
   "source": [
    "def results_to_dataframe(results):\n",
    "    data = []\n",
    "    for rule, result in results:\n",
    "        if result.get('success', False):\n",
    "            data.append({\n",
    "                'Rule Name': rule['name'],\n",
    "                'Rule ID': rule['asset_id'],\n",
    "                'Status': result.get('status', 'unknown'),\n",
    "                'Job ID': result.get('job_id', 'N/A'),\n",
    "                'Job Run ID': result.get('job_run_id', 'N/A'),\n",
    "                'Success': True,\n",
    "                'Error': None\n",
    "            })\n",
    "        else:\n",
    "            data.append({\n",
    "                'Rule Name': rule['name'],\n",
    "                'Rule ID': rule['asset_id'], \n",
    "                'Status': 'Failed',\n",
    "                'Job ID': 'N/A',\n",
    "                'Job Run ID': 'N/A',\n",
    "                'Success': False,\n",
    "                'Error': result.get('error', 'Unknown error')\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "965dd05e-f68e-4f48-a09e-84ee68bbade6"
   },
   "source": [
    "## Save Results\n",
    "\n",
    "Saves API calls result as a data asset withtin the project as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9e1a5c6e-1703-49f5-a087-c8449cbdc901"
   },
   "outputs": [],
   "source": [
    "def save_results(pandas_df, prefix=\"dq_run\"):\n",
    "    \"\"\"Save DataFrame with timestamp in filename\"\"\"\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    filename = f\"{prefix}_{timestamp}.csv\"\n",
    "    \n",
    "    # Save the file\n",
    "    wslib.save_data(filename, pandas_df.to_csv(index=False).encode())\n",
    "    print(f\"Results saved to: {filename}\")\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e950b752-3d7d-4705-a5a7-e845d45249cf"
   },
   "source": [
    "## Execute Rules and Save Results\n",
    "\n",
    "Main execution block: get current project ID and auth token, run rules in parallel, process results, and save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d1963b67-c1aa-4cbf-bdc6-f9b43b1976c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: dq_run_2025-06-26-04-02-20.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dq_run_2025-06-26-04-02-20.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_id = wslib.here.get_ID()\n",
    "token = wslib.auth.get_current_token()\n",
    "\n",
    "results = execute_rules_parallel_with_delay(token, project_id, dq_rules, batch_size=BATCH_SIZE, delay_between_batches=DELAY_IN_SECONDS)\n",
    "df = results_to_dataframe(results)\n",
    "save_results(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bacebc16-0083-4e3b-a648-fdbe9581138a"
   },
   "source": [
    "## Display Results (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45ae923d-79dc-4c5b-b0db-5dd681d964fa"
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
